{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c814ff",
   "metadata": {},
   "source": [
    "## 파이토치 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346bb2e7",
   "metadata": {},
   "source": [
    "### 1. 파이토치로 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c5385",
   "metadata": {},
   "source": [
    "#### 선형회귀 가설식\n",
    "\n",
    "$$\n",
    "H(x) = Wx+b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0709af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14b37d615d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8690386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 (y = 2x)\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88f528d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 선언 및 초기화\n",
    "model = nn.Linear(1, 1) # (input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4661778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.8362]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0177], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 모델의 초기화된 가중치와 편향 출력\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e041e7f",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "#### 1) 옵티마이저 - 경사 하강법(Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cedd6e2",
   "metadata": {},
   "source": [
    "* 손실 함수(Loss function): 일반적으로 평균 제곱 오차 (Mean Squared Error, MSE) 사용\n",
    "* 손실 함수(Loss function) = 비용 함수(Cost function) = 오차 함수(Error function) = 목적 함수 (Objective function)\n",
    "\n",
    "<center>\n",
    "\n",
    "$L(w,b)= \\sum_{i=1}^{n} (y^{(i)}-\\hat{y}^{(i)})^2$\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83be4e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/10-1.GD.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/10-1.GD.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135b40d",
   "metadata": {},
   "source": [
    "$$\n",
    "gradient = \\frac{\\partial L(W)}{\\partial W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc21983",
   "metadata": {},
   "source": [
    "* 기울기가 음수일 때 (Negative gradient): W의 값이 증가\n",
    "\n",
    "$$\n",
    "W := W-\\alpha \\times (-gradient) = W + \\alpha \\times gradient\n",
    "$$\n",
    "\n",
    "* 기울기가 양수일 때 (Positive gradient): W의 값이 감소\n",
    "\n",
    "$$\n",
    "W := W-\\alpha \\times gradient\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c90240d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/10-1.optimizer.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다양한 최적화 알고리즘들\n",
    "# [출처] https://ganghee-lee.tistory.com/24\n",
    "Image(url='https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/10-1.optimizer.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1bcec",
   "metadata": {},
   "source": [
    "##### 자동 미분 (Autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85a4d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = w**2\n",
    "z = 2 * y + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "891b9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc649606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31592b2",
   "metadata": {},
   "source": [
    "#### 2) 데이터 로드\n",
    "\n",
    "전체 데이터를 하나의 행렬로 선언하여 전체 데이터에 대해서 경사 하강법을 수행하여 학습할 수 있음. 만약, 데이터가 수십만개 이상이라면 전체 데이터에 대해서 경사 하강법을 수행하는 것은 매우 느릴 뿐만 아니라 많은 계산량을 필요로 함. 따라서 전체 데이터를 더 작은 단위로 나누어서 해당 단위로 학습하는 방식을 사용함.\n",
    "\n",
    "* 에포크(Epoch): 전체 훈련 데이터가 학습에 한 번 사용된 주기\n",
    "* 미니배치(minibatch): 전체 데이터를 나누는 단위\n",
    "* 배치 크기: 미니 배치의 크기\n",
    "    - 보통 2의 제곱수 사용\n",
    "* 이터레이션(Iteration): 한 번의 에포크 내에서 이루어지는 매개변수 $(W$와 $b)$ 업데이트 횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0410df7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/10-1.data.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/10-1.data.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78e944",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf10b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정. 경사 하강법 SGD를 사용하고 학습률(learning rate, lr)은 0.01로 설정\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d6aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 훈련 데이터에 대해 경사 하강법을 2000회 반복\n",
    "\n",
    "# 총 에포크 수 설정\n",
    "n_epochs = 2000\n",
    "for epoch in range(n_epochs + 1):\n",
    "    # 1) 가설모델 H(x)=Wx+b 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # 2) loss 계산\n",
    "    loss = F.mse_loss(prediction, y_train)  # <-- 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # 3) loss로 H(x) 개선\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 손실 함수를 미분하여 gradient 계산\n",
    "    loss.backward()\n",
    "    # W, b 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Loss: {:.6f}'.format(\n",
    "            epoch, n_epochs, loss.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3523b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임의의 입력 4를 선언\n",
    "new_var =  torch.FloatTensor([[4.0]]) \n",
    "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) # forward 연산\n",
    "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
    "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b3540",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e93bb4",
   "metadata": {},
   "source": [
    "* forward 연산: $H(x)$ 식에 입력 $x$로부터 예측된 $y$를 얻는 것\n",
    "    - ex) 학습전 ```prediction = model(x_train)```, 학습후 ```pred_y = model(new_var)```\n",
    "* backward 연산: 학습 과정에서 비용 함수를 미분하여 기울기를 구하는 것\n",
    "    - ex) ```cost.backward()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3020bcc",
   "metadata": {},
   "source": [
    "### 2. 파이토치로 로지스틱 회귀 구현하기\n",
    "\n",
    "#### 로지스틱 회귀의 가설식\n",
    "\n",
    "$$\n",
    "H(x) = sigmoid(Wx+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fde164",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e356d3",
   "metadata": {},
   "source": [
    "* ```nn.Sequential()```: 여러 함수들을 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633ce02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 1), # input_dim=2, output_dim=1\n",
    "    nn.Sigmoid() # 출력은 시그모이드 함수를 거친다\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b2da1",
   "metadata": {},
   "source": [
    "-------------------\n",
    "#### 1) 손실 함수(Loss Function)\n",
    "\n",
    "**평균 제곱 오차(MSE)를 로지스틱 회귀의 비용함수로 사용한다면?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d642d51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/10-1.sigmoid_mse.png\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/10-1.sigmoid_mse.png', width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab2b2f",
   "metadata": {},
   "source": [
    "시그모이드 함수의 출력값은 0에서 1사이. 즉, 실제값이 1일 때 예측값이 0에 가까와지면 오차 증가, 실제값이 0일 때 예측값이 1에 가까워지면 오차가 커져야 함.\n",
    "\n",
    "$$\n",
    "if y=1 \\rightarrow L(H(x),y)=-log(H(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "if y=0 \\rightarrow L(H(x),y)=-log(1-H(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdb72a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/10-1.cross_entropy.png\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/10-1.cross_entropy.png', width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02f570e",
   "metadata": {},
   "source": [
    "하나의 식으로 통합\n",
    "\n",
    "$$\n",
    "L(H(x),y) = -[ylogH(x)+(1-y)log(1-H(x))]\n",
    "$$\n",
    "\n",
    "$$\n",
    "L(W)=-\\frac{1}{n} \\sum_{i=1}^n [y^{(i)} logH(x^{(i)}) + (1-y^{(i)}) log(1-H(x^{(i)}))]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19962615",
   "metadata": {},
   "source": [
    "**크로스 엔트로피 함수 (Cross Entropy Function)**\n",
    "\n",
    "* $k$개의 다중 클래스 분류로 일반화\n",
    "\n",
    "$$\n",
    "L(W)=-\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^k y_j log(p_j)\n",
    "$$\n",
    "\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a143228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    loss = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 10번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예측값이 0.5를 넘으면 True로 간주\n",
    "        correct_prediction = prediction.float() == y_train # 실제값과 일치하는 경우만 True로 간주\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에포크마다 정확도를 출력\n",
    "            epoch, nb_epochs, loss.item(), accuracy * 100,\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e843c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

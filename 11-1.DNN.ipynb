{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31adb033",
   "metadata": {},
   "source": [
    "## 파이토치로 다층 퍼셉트론 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee8b5dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/9-2.DNN.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/9-2.DNN.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3f28088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5449c7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "865f7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 연산이 가능하다면 GPU를 사용하고, 불가능하다면 CPU를 사용\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa83b9",
   "metadata": {},
   "source": [
    "### 1) 다층 퍼셉트론을 이용한 XOR 문제 풀기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3664725",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor([[0,0],[0,1],[1,0],[1,1]]).to(device)\n",
    "Y = torch.FloatTensor([[0],[1],[1],[0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb12de6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/11-1.DNN.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/11-1.DNN.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ccc3140",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 10, bias=True), # input_layer = 2, hidden_layer1 = 10\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10, 10, bias=True), # hidden_layer1 = 10, hidden_layer2 = 10\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10, 10, bias=True), # hidden_layer2 = 10, hidden_layer3 = 10\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(10, 1, bias=True), # hidden_layer3 = 10, output_layer = 1\n",
    "    nn.Sigmoid()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff9667d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcbd97e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000 Cost: 0.694898\n",
      "Epoch 100/10000 Cost: 0.693156\n",
      "Epoch 200/10000 Cost: 0.693154\n",
      "Epoch 300/10000 Cost: 0.693151\n",
      "Epoch 400/10000 Cost: 0.693149\n",
      "Epoch 500/10000 Cost: 0.693147\n",
      "Epoch 600/10000 Cost: 0.693145\n",
      "Epoch 700/10000 Cost: 0.693143\n",
      "Epoch 800/10000 Cost: 0.693142\n",
      "Epoch 900/10000 Cost: 0.693140\n",
      "Epoch 1000/10000 Cost: 0.693138\n",
      "Epoch 1100/10000 Cost: 0.693136\n",
      "Epoch 1200/10000 Cost: 0.693134\n",
      "Epoch 1300/10000 Cost: 0.693132\n",
      "Epoch 1400/10000 Cost: 0.693130\n",
      "Epoch 1500/10000 Cost: 0.693128\n",
      "Epoch 1600/10000 Cost: 0.693126\n",
      "Epoch 1700/10000 Cost: 0.693124\n",
      "Epoch 1800/10000 Cost: 0.693122\n",
      "Epoch 1900/10000 Cost: 0.693120\n",
      "Epoch 2000/10000 Cost: 0.693117\n",
      "Epoch 2100/10000 Cost: 0.693115\n",
      "Epoch 2200/10000 Cost: 0.693112\n",
      "Epoch 2300/10000 Cost: 0.693109\n",
      "Epoch 2400/10000 Cost: 0.693105\n",
      "Epoch 2500/10000 Cost: 0.693101\n",
      "Epoch 2600/10000 Cost: 0.693097\n",
      "Epoch 2700/10000 Cost: 0.693093\n",
      "Epoch 2800/10000 Cost: 0.693088\n",
      "Epoch 2900/10000 Cost: 0.693083\n",
      "Epoch 3000/10000 Cost: 0.693076\n",
      "Epoch 3100/10000 Cost: 0.693069\n",
      "Epoch 3200/10000 Cost: 0.693061\n",
      "Epoch 3300/10000 Cost: 0.693052\n",
      "Epoch 3400/10000 Cost: 0.693041\n",
      "Epoch 3500/10000 Cost: 0.693028\n",
      "Epoch 3600/10000 Cost: 0.693013\n",
      "Epoch 3700/10000 Cost: 0.692995\n",
      "Epoch 3800/10000 Cost: 0.692973\n",
      "Epoch 3900/10000 Cost: 0.692945\n",
      "Epoch 4000/10000 Cost: 0.692910\n",
      "Epoch 4100/10000 Cost: 0.692865\n",
      "Epoch 4200/10000 Cost: 0.692805\n",
      "Epoch 4300/10000 Cost: 0.692722\n",
      "Epoch 4400/10000 Cost: 0.692604\n",
      "Epoch 4500/10000 Cost: 0.692428\n",
      "Epoch 4600/10000 Cost: 0.692148\n",
      "Epoch 4700/10000 Cost: 0.691666\n",
      "Epoch 4800/10000 Cost: 0.690740\n",
      "Epoch 4900/10000 Cost: 0.688621\n",
      "Epoch 5000/10000 Cost: 0.682082\n",
      "Epoch 5100/10000 Cost: 0.647257\n",
      "Epoch 5200/10000 Cost: 0.449915\n",
      "Epoch 5300/10000 Cost: 0.041613\n",
      "Epoch 5400/10000 Cost: 0.009754\n",
      "Epoch 5500/10000 Cost: 0.005039\n",
      "Epoch 5600/10000 Cost: 0.003298\n",
      "Epoch 5700/10000 Cost: 0.002417\n",
      "Epoch 5800/10000 Cost: 0.001892\n",
      "Epoch 5900/10000 Cost: 0.001547\n",
      "Epoch 6000/10000 Cost: 0.001303\n",
      "Epoch 6100/10000 Cost: 0.001123\n",
      "Epoch 6200/10000 Cost: 0.000985\n",
      "Epoch 6300/10000 Cost: 0.000875\n",
      "Epoch 6400/10000 Cost: 0.000787\n",
      "Epoch 6500/10000 Cost: 0.000714\n",
      "Epoch 6600/10000 Cost: 0.000653\n",
      "Epoch 6700/10000 Cost: 0.000601\n",
      "Epoch 6800/10000 Cost: 0.000556\n",
      "Epoch 6900/10000 Cost: 0.000518\n",
      "Epoch 7000/10000 Cost: 0.000484\n",
      "Epoch 7100/10000 Cost: 0.000454\n",
      "Epoch 7200/10000 Cost: 0.000427\n",
      "Epoch 7300/10000 Cost: 0.000404\n",
      "Epoch 7400/10000 Cost: 0.000382\n",
      "Epoch 7500/10000 Cost: 0.000363\n",
      "Epoch 7600/10000 Cost: 0.000345\n",
      "Epoch 7700/10000 Cost: 0.000329\n",
      "Epoch 7800/10000 Cost: 0.000315\n",
      "Epoch 7900/10000 Cost: 0.000301\n",
      "Epoch 8000/10000 Cost: 0.000289\n",
      "Epoch 8100/10000 Cost: 0.000278\n",
      "Epoch 8200/10000 Cost: 0.000267\n",
      "Epoch 8300/10000 Cost: 0.000257\n",
      "Epoch 8400/10000 Cost: 0.000248\n",
      "Epoch 8500/10000 Cost: 0.000239\n",
      "Epoch 8600/10000 Cost: 0.000231\n",
      "Epoch 8700/10000 Cost: 0.000224\n",
      "Epoch 8800/10000 Cost: 0.000217\n",
      "Epoch 8900/10000 Cost: 0.000210\n",
      "Epoch 9000/10000 Cost: 0.000204\n",
      "Epoch 9100/10000 Cost: 0.000198\n",
      "Epoch 9200/10000 Cost: 0.000192\n",
      "Epoch 9300/10000 Cost: 0.000187\n",
      "Epoch 9400/10000 Cost: 0.000182\n",
      "Epoch 9500/10000 Cost: 0.000177\n",
      "Epoch 9600/10000 Cost: 0.000173\n",
      "Epoch 9700/10000 Cost: 0.000168\n",
      "Epoch 9800/10000 Cost: 0.000164\n",
      "Epoch 9900/10000 Cost: 0.000160\n",
      "Epoch 10000/10000 Cost: 0.000157\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    hypothesis = model(X)\n",
    "\n",
    "    # 비용 함수 계산 및 역전파\n",
    "    cost = criterion(hypothesis, Y)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100 epoch마다 loss 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}/10000 Cost: {cost.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "446186a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ab98c16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델의 출력값(Hypothesis): \n",
      " [[1.1170164e-04]\n",
      " [9.9982882e-01]\n",
      " [9.9984229e-01]\n",
      " [1.8532644e-04]]\n",
      "모델의 예측값(Predicted): \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "실제값(Y): \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "정확도(Accuracy): \n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hypothesis = model(X)\n",
    "    predicted = (hypothesis > 0.5).float()\n",
    "    accuracy = (predicted == Y).float().mean()\n",
    "    print('모델의 출력값(Hypothesis): \\n', hypothesis.detach().cpu().numpy())\n",
    "    print('모델의 예측값(Predicted): \\n', predicted.detach().cpu().numpy())\n",
    "    print('실제값(Y): \\n', Y.cpu().numpy())\n",
    "    print('정확도(Accuracy): \\n', accuracy.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4af364",
   "metadata": {},
   "source": [
    "* ```with torch.no_grad()```: 기울기 계산을 비활성화 하여 연산 속도 증가 및 메모리 사용량 감소\n",
    "* ```.detach()```: 계산 그래프에서 분리\n",
    "* ```.cpu()```: cpu로 보냄 (= ```.to('cpu')```)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7470a",
   "metadata": {},
   "source": [
    "### 2) 다층 퍼셉트론으로 MNIST 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c77942",
   "metadata": {},
   "source": [
    "#### MNIST 데이터\n",
    "\n",
    "* MNIST: 숫자 0부터 9까지의 이미지로 구성된 손글씨 데이터셋\n",
    "    - 6만개의 훈련 데이터와 레이블, 1만개의 테스트 데이터와 레이블로 구성\n",
    "    - 레이블: 0 부터 9 (10개)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae764700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/11-1.mnist.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/11-1.mnist.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ee66a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/11-1.mnist_input.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각각의 이미지는 28x28 크기의 픽셀\n",
    "Image(url='https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/11-1.mnist_input.png', width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fb0dcc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/11-1.mnist_input.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 다층 퍼셉트론의 input으로 사용하기 위해 28x28 크기의 2차원 배열을 784 크기의 1차원 배열로 변환\n",
    "Image(url='https://raw.githubusercontent.com/Hyelim-Shin/AIdev/master/Images/11-1.mnist_input.png', width=500)\n",
    "\n",
    "## 코드 참고\n",
    "# for X, Y in data_loader:\n",
    "#     X = X.view(-1, 28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ea4c6",
   "metadata": {},
   "source": [
    "#### MNIST 분류기 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb2325",
   "metadata": {},
   "source": [
    "##### i) 사전 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ea628b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e889a0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3215cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65111454",
   "metadata": {},
   "source": [
    "##### ii) MNIST 데이터 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0261022",
   "metadata": {},
   "source": [
    "--------------------\n",
    "**DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa85b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee6d2c",
   "metadata": {},
   "source": [
    "TensorDataset은 기본적으로 텐서를 입력으로 받음. 텐서 형태로 데이터 정의."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be9cf841",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
    "                               [93,  88,  93], \n",
    "                               [89,  91,  90], \n",
    "                               [96,  98,  100],   \n",
    "                               [73,  66,  70]])  \n",
    "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
    "\n",
    "dataset = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6dfb308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0c954",
   "metadata": {},
   "source": [
    "* ```batch_size```: 미니 배치의 크기\n",
    "* ```shuffle```: Epoch마다 데이터셋을 섞어 데이터가 학습되는 순서를 바꿈. 순서가 학습에 영향을 주는 것을 방지\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8420888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f28fbf",
   "metadata": {},
   "source": [
    "* ```torchvision```: 유명한 데이터셋들, 이미 구현되어져 있는 유명한 모델들, 일반적인 이미지 전처리 도구들을 포함하고 있는 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97789963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MNIST dataset\n",
    "mnist_train = datasets.MNIST(root=\"Data\",              # 데이터가 저장될 경로\n",
    "                            train=True,                         # True면 훈련 데이터, False면 테스트 데이터\n",
    "                            transform=transforms.ToTensor(),    # 이미지 데이터를 텐서로 변환\n",
    "                            download=True)                      # 데이터가 없다면 다운로드 (데이터가 있는 경우 설정 필요 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "deb37b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,\n",
       "          18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "         253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253, 253,\n",
       "         253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253, 253,\n",
       "         198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253, 205,\n",
       "          11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,  90,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253, 190,\n",
       "           2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,\n",
       "          70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35, 241,\n",
       "         225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  81,\n",
       "         240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "         229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221, 253,\n",
       "         253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253, 253,\n",
       "         253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253, 195,\n",
       "          80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,  11,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "       dtype=torch.uint8)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98fdb1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "facd52a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_train.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd34dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bfc8f453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cf59fee7d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = mnist_train.data[0]\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1224ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist_train.data.view((-1, 28**2)) / 255.0 # 정규화, matrix to vector\n",
    "y = mnist_train.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1c83849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X, y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54574c96",
   "metadata": {},
   "source": [
    "* ```drop_last```: 마지막 배치는 다른 미니 배치보다 개수가 적을 수 있음. 이 미니 배치를 버릴지 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2461eca",
   "metadata": {},
   "source": [
    "##### iii) 다층 퍼셉트론 설계 및 학습\n",
    "\n",
    "784(28x28)개의 입력 뉴런, 10개의 출력 뉴런(0~9)을 가지는 다층 퍼셉트론을 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a95f63fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bcac62da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(28**2, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "702c04a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차함수 선택\n",
    "loss_fn = nn.CrossEntropyLoss().to(device) # 내부적으로 softmax 포함\n",
    "\n",
    "# 최적화 기법 선택\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3203b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 0.2868\n",
      "Epoch 2/3, Loss: 0.2206\n",
      "Epoch 3/3, Loss: 0.2476\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data, targets in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data.to(device))\n",
    "        loss = loss_fn(y_pred, targets.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023afc07",
   "metadata": {},
   "source": [
    "##### iv) 테스트 데이터에 대해 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e60a9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = datasets.MNIST(root=\"Data\",\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "X_test = mnist_test.data.view((-1, 28**2)) / 255.0\n",
    "y_test = mnist_test.targets\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a2b7dda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 데이터에서 예측 정확도: 9651/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()    # 신경망을 추론 모드로 전환\n",
    "\n",
    "correct = 0\n",
    "\n",
    "# 데이터로더에서 미니배치를 하나씩 꺼내 추론을 수행\n",
    "with torch.no_grad():  # 추론 과정에는 미분이 필요없음\n",
    "    for data, targets in test_dataloader:\n",
    "        outputs = model(data.to(device))  # 데이터를 입력하고 출력을 계산 (128x10)\n",
    "\n",
    "        # 추론 계산\n",
    "        _, predicted = torch.max(outputs, dim=1)  # 확률이 가장 높은 레이블이 무엇인지 계산 (argmax)\n",
    "        correct += (predicted == targets.to(device).view_as(predicted)).sum()  # 정답과 일치한 경우 정답 카운트를 증가\n",
    "\n",
    "# 정확도 출력\n",
    "data_num = len(test_dataloader.dataset)  # 데이터 총 건수\n",
    "print('\\n테스트 데이터에서 예측 정확도: {}/{} ({:.0f}%)\\n'.format(correct, data_num, 100. * correct / data_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c4631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

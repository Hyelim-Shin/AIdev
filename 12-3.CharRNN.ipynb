{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37dc4506",
   "metadata": {},
   "source": [
    "## 문자 단위 RNN(Char RNN)\n",
    "\n",
    "### 1. 문자 단위 RNN (Char RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee836b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ee2e7",
   "metadata": {},
   "source": [
    "#### 1) 훈련 데이터 전처리하기\n",
    "\n",
    "구현할 것 : ```apple``` $\\rightarrow$ ```pple!```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f00d89",
   "metadata": {},
   "source": [
    "##### 입력 데이터와 레이블 데이터에 대해서 문자 집합(vocabulary)을 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bed5c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dfbe2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', 'a', 'e', 'l', 'p']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6773e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c26296a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "425f815d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "index_to_char = dict((i, c) for c, i in char_to_index.items()) # 문자에 고유한 정수 인덱스 부여\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5752163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4af7de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n"
     ]
    }
   ],
   "source": [
    "# 배치 차원 추가\n",
    "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3c289cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 원-핫 인코딩\n",
    "x_one_hot = np.array([np.eye(vocab_size)[x] for x in x_data])\n",
    "print(x_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc08fde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f642f332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab480a",
   "metadata": {},
   "source": [
    "#### 2) 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21dde1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "\n",
    "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfe15121",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79a3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(X)\n",
    "print(outputs.shape)  # batch_size, sequence_length, output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be24a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4c2d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.7317272424697876 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
      "1 loss:  1.4012726545333862 prediction:  [[4 4 2 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppeee\n",
      "2 loss:  1.2407243251800537 prediction:  [[4 4 3 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplee\n",
      "3 loss:  1.074204444885254 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "4 loss:  0.9040333032608032 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "5 loss:  0.7173529863357544 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "6 loss:  0.5446842908859253 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "7 loss:  0.4152960777282715 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.3083057403564453 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.2193511724472046 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.15346698462963104 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.10785669088363647 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.07643942534923553 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.0546867735683918 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.03973376005887985 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.029460635036230087 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.02232508361339569 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.017289070412516594 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.013672309927642345 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.011027645319700241 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.009058801457285881 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.007567179389297962 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.006418383214622736 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.005519661121070385 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.0048066722229123116 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.004233698360621929 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.0037674803752452135 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.003384290961548686 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.0030661560595035553 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.0027995735872536898 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.002574463840574026 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.0023829408455640078 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.0022188429720699787 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.002077333629131317 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.001954594161361456 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.001847587525844574 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.0017538204556331038 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.0016712987562641501 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.0015983361518010497 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.0015335542848333716 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.0014758367324247956 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.0014241375029087067 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.0013777673011645675 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0013360127341002226 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.0012983034830540419 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0012641397770494223 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.0012330698082223535 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.0012047367636114359 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.00117878383025527 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0011550921481102705 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.001133257057517767 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.0011131358332931995 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.001094609615392983 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.0010773688554763794 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.0010614374186843634 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.001046553603373468 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0010326460469514132 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.0010196190560236573 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.0010074733290821314 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.000995970331132412 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.0009851816575974226 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.0009749404853209853 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.0009652471053414047 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.0009560537291690707 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.000947265129070729 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.0009389527840539813 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.0009309500455856323 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0009232329321093857 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.0009159683249890804 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.0009088704246096313 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.0009020344587042928 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.000895460310857743 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.0008890531025826931 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.0008829315192997456 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.0008769050473347306 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.0008710691472515464 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.000865400128532201 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.0008598977001383901 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.0008543951553292572 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0008491069893352687 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.0008439378580078483 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0008388640126213431 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.0008338854531757534 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0008289306424558163 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.0008241664618253708 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.000819426029920578 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.0008147332118824124 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0008100879495032132 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0008055856451392174 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0008011309546418488 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.0007967000128701329 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.0007923167431727052 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.0007880048942752182 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.0007837407174520195 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.000779524096287787 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.0007753552054055035 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.0007712100050412118 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.0007671363418921828 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.0007630864274688065 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.0007591079920530319 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs.view(-1, output_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
    "\n",
    "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d7f12d",
   "metadata": {},
   "source": [
    "### 2. 더 많은 데이터로 학습한 문자 단위 RNN (Char RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9677c",
   "metadata": {},
   "source": [
    "#### 1) 훈련 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50bbea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55186331",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
    "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba7ea706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'o': 1, 'n': 2, 'm': 3, 'i': 4, 'd': 5, 'f': 6, 'c': 7, 'b': 8, 'p': 9, '.': 10, 'a': 11, 't': 12, 'y': 13, 'g': 14, 'r': 15, 'l': 16, 'e': 17, 's': 18, 'w': 19, ',': 20, 'k': 21, 'u': 22, \"'\": 23, 'h': 24}\n"
     ]
    }
   ],
   "source": [
    "print(char_dic) # 공백도 여기서는 하나의 원소\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df94077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 25\n"
     ]
    }
   ],
   "source": [
    "dic_size = len(char_dic)\n",
    "print('문자 집합의 크기 : {}'.format(dic_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "022c6043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10  # 임의 숫자 지정\n",
    "learning_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "295ceeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
    "    y_data.append([char_dic[c] for c in y_str])  # y str to index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da830366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 0, 13, 1, 22, 0, 19, 11, 2]\n",
      "[6, 0, 13, 1, 22, 0, 19, 11, 2, 12]\n"
     ]
    }
   ],
   "source": [
    "print(x_data[0])    # if you wan\n",
    "print(y_data[0])    # f you want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a23d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd71fd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([170, 10, 25])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d61ed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([ 6,  0, 13,  1, 22,  0, 19, 11,  2, 12])\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d404f51",
   "metadata": {},
   "source": [
    "#### 2) 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2701202",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "75438850",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55796d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "885f0443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kkskkskskkkkkkskkkksssssskkkksskkksslskkkkskkskkkskkskksosskksskskkskksksskskksskkkkssskkkksksoskkksskkskkssksskkskkkosksssskkssskkskskkksskkskkksoksssskkskskkkkskkkkkokskkkkossks\n",
      "  ee       e een  e e   e      ee    ee      eee   ee  e  e ee  ee   ee       e  ee    eee e e ee         e      e  e   e  e    e  e ee  ee ee    t e     e  et  ee  e e     e  e  \n",
      "o  o  o    to    o  to      t   o o  o  o   o  o           o to o    t     o  t     oo  o  o  o  o o  to  t   o t    o      o o  o  o to o   o o  t    o  t   t   o     o  o  o  o \n",
      "o .o i..do  o oo e   o oei  eoeo  eoeo  e e  e e  oe e emeo  io e  e eoeo  e e oeie eo oe eo  o  o i  eoe eoeoe eoe eoe emeo     e eo  o io eoeo i  eodo ieoeiaoe e e o ie eoeoeeei\n",
      "  e e e e e e me ee e e eme e eme ememe o  ee  e  ee e eme e me  e e eme e oe e eme eme e   e me n  e eme e e e eoe e e eme e  e   eme n  e  e oe e ememe eme e e e ee eme e eme e \n",
      "    d ne n do  nd d n  o o   n l  ln l do do  le e  o   l lo  n  d n  o   do  n  l   n l   n  o  o    un lde e e n ldl   a do  n l  o  o  n  d o  n  n o e n  ah l   d  lh  h o  oe\n",
      "lh  d dost wostnstd wnstdo tdo tnlao thdostmo tn thg  t o to tn tn   wo t wod do ts wo tmn tnso tos d dos do td dostdosldo tostn to o tostn to o tdososo o o tao tnd wo tn tososedo\n",
      "to ld t t gto ta t ttmn tat tht t tht t t ttt ttntls t tnt osto tec ltnt taos wot t tn  to toso tos tltnt to  t t t tod tnt oncn thsth osta t tos tht t t tnt tn ttn to to thsct t \n",
      "to tt t t tto to t tt thtct to  t th  ttt tth tt tht t t  to to te   to  ttos tot t to tto thto ton t tnt to  t tot tot trttonto thtt to to t to  thtct t t  tto ttn to to thtct t \n",
      "to 'd tht  to to t  tod tct tos't to c tm  th to the   tr to to te t to  tton wo 't to tto thim to  s tnt ta  t tot tms tm to to them to to t ton t emt t tm  tm ttn to to themt t \n",
      "'oe'd toeo touto te wmd emt ion's to m tms to te the   tm to to te t tm   tmn wou's dm  ta toem ton s wmd dau t tme tms em to to them to to t tmn w emumd tm  am  ru to to themu r \n",
      "poepd aoe  to loeme amd emt don   ao m tm ero lenthe e em to lo be t do   amn wou't dm epa toem tos   ams wou t dme dms em to lo them to lo b aor woemu d em eam emu to ao toemu e \n",
      "poent aoem eorluepe aodhem, aor t aoem to er  le the e er to lo le   do   aor wor t do egy lhem toe   ans wor t aoe dos em to l ethem to lo l eor whemsod eo  ao  m  go pu thems e \n",
      "p, a, aosl torluepu to,hep, wor't ao p do loh le th    en to lh len  do l aos wor't an egy lhen to ls tns wor , aos dnshem to l  then to lo l eor themsod eos aos ns go pu thems e \n",
      "pytus ao c to lurle to,hep, aor't ao c do leh le th  t en to lo le t tn l and aor't an egu lhen to l  tns wor', tn  tnsheo to l  then to le l los theosnt eos dos ns go pu theo  e \n",
      "gotud ao c to cunlu tothep, aos't ao m to len le th  the  to lo le t to l and aos't an egu theo to    tns wor', tn  tnsheo to c  them to lesl los theo nt eos dos ns go pu theo  e \n",
      "gotud tosc to cu lu tot ep, aon't aosm to tensle thr t em torco lect wo c and aon't an egu them ton t tnd wor', ao  dnthem tonc  them to cesl don them nd ens uosens gy tu themdee \n",
      "gotud tosc to cu ld tod em,hwor't ao m to tensle the them to co lect wo c and aor't an egn them tos   tnd dorc, an  d'them tosc  them to cenl don them nd eos uosens gy tf them eec\n",
      "potun tosc do cu ly aonhem,hwon't do m to teos'e the them to co lect wo l wnd don't dns gn them toskt tnd dork, an  aathem tosco them to ce l don themdnt ems dosens ty tf themdea \n",
      "potan tont do cu ld aonhem,hwon't do m to tensle the them to lo le t wo l wnd don't dns gn them ton t tnd dork, dn  aathem tonco them to leng don them nd ems dosens ty tf themtea \n",
      "potan tant to cueld aanhem,hwon't do m to te s'e tha them to lolle t wo l wnd dor't dns gn them toakt tnd dork, du  aathem to co them to leng for them nd ems dosens ty pn them eas\n",
      "potan tant to cueld aadhip, won't do m uo te sle tha them to collect wo k and dor't dns gn them to kt tnd dork, duthaathem to co them to long for them nd ess dosens ty pn themt as\n",
      "loeon tant to lueld aadhim, won't doum uo teosle tha them to lollect wo l wnd don't dns gn them to kt tnd dork, but aathem to co them to long fon them nd ess dosens ty pn themseac\n",
      "loeon tont to lueld a ship, won't doum uo teosle thn them to lollect wo f and don't dns gn them tosks tnd dork, but dathem tosch them to long for'them nd ess dosens ty pf themseac\n",
      "loeon tont to bueld aoship, don't doum to teosle thg ther to bollect woof and don't dnsign them tosks tnd dork, but rather toach them to bong for ther nd ess iohens ty pf them eac\n",
      "lorun tont to bueld aoship, don't doum fo teosle thn ther to bollect worc and don't dnsign them tosks bnd dork, but rather toach them to bong for therend ess iosens ty bf themseac\n",
      "lorou tont to build aoship, don't doum fo peosle tog ther to bollect ao d and don't dnsign them tosks and dork, but rather toach them to bong for therend ensiirsensity of therseac\n",
      "porou want to build a ship, don't doum uo peosle tog ther to bollect ao d and don't dnsign them tasks and work, but rather toach them ta bong for themend ensiimmensity of therseac\n",
      "porou want to cuild a ship, don't doum uo peosle tog ther to collect word and won't dnsign them tasks and work, but rather toach them ta cong for themtnd ens immens ty of themseac\n",
      "pmrou want to cuild a ship, don't doum uu peoele together to collect word and won't dnsign them tosks and work, but rather toach them to cong for themtndlens immens ty of themseac\n",
      "pmrpu want to luild a ship, don't doum up people together te lollect word and won't dnsign them tosks and work, but rather teach them to long for themsndlens immens ty of themseas\n",
      "porou want to luild a ship, don't doum up people thgether te lollect word and won't dnsign ther tosks and work, but rather teach ther to long for therendlens immensity of thereean\n",
      "porou want to luild a ship, don't doum up people together to lollect word and don't dnsign ther tasks and work, but rather toach them ta long for themendlens immensity of thereean\n",
      "pmrou want to luild a ship, don't doum up people together te lollect wood and don't dnsign them tosks and work, but rather toach them to long for the endless immensity of themeean\n",
      "pmroo want to luild a ship, don't doum up people together te lollect wood and don't dnsign them tosks and work, but rather teach them to cong for the endless immensity of the eeas\n",
      "mmrou want to luild a ship, don't drum up people together to collect wood and don't dnsign them tosks and work, but rather toach them to cong for the endless immensity of the eean\n",
      "mmrou want to build a ship, don't drum up people together to collect wood and won't dnsign ther tasks and work, but rather teach them ta cong for the endless immensity of the eean\n",
      "mmrou want to build a ship, don't arum up people together te bollect wood and don't assign them tasks and work, but rather teach them ta cong for the endless immensity of the e as\n",
      "mmrou want to build a ship, don't arum up people together to bollect wood and don't assign them tosks and work, but rather teach them to bong for the endless immensity of the eeas\n",
      "mmrou want to build a ship, don't drum up people together to bollect wood and don't dssign them tosks and work, but rather teach them to bong for the endless immensity of the eean\n",
      "mmrou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta cong for the endless immensity of the eean\n",
      "mmrou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta cong for the endless immensity of the eean\n",
      "pmrou want to build a ship, don't arum up people together to lollect wood and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the eean\n",
      "pmrou want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to cong for the endless immensity of the eean\n",
      "pmyou want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them ta cong for the endless immensity of the eean\n",
      "pmyou want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to cong for the endless immensity of the eean\n",
      "pmyou want to build a ship, don't drum up people together te bollect wood and don't dssign them tosks and work, but rather teach them to bong for the endless immensity of the eeac\n",
      "mmyou want to build a ship, don't arum up people together to bollect wood and don't assign them tosks and work, but rather teach them to bong for the endless immensity of the eea.\n",
      "mmyou want to build a ship, don't drum up people together to lollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "mmyou want to build a ship, don't arum up people together te lollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "mmyou want to build a ship, don't drum up people together to lollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "mmyou want to build a ship, don't drum up people together to lollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "lmyou want to build a ship, don't arum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "lmyou want to build a ship, don't drum up people together te lollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "lmyou want to build a ship, don't drum up people together to lollect wood and don't assign them tosks and work, but rather teach them to long for the sndless immensity of the sea.\n",
      "lmyou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "lmyou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea.\n",
      "lmyou want to build a ship, don't drum up people together te collect wood and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the eea.\n",
      "pmyou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the sndless immensity of the sea.\n",
      "p you wont to build a ship, don't drum up people together te collect wood and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the eea.\n",
      "p you want to build a ship, don't drum up people together te lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eeas\n",
      "m you want to build a ship, don't drum up people together to bollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sean\n",
      "m you want to luild a ship, don't arum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to luild a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to lollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the sea.\n",
      "g you want to build a ship, don't arum up people together to bollect wood and don't assign them tasks and work, but rather teach them ta cong for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta cong for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to cong for the endless immensity of the sea.\n",
      "p you want to build a ship, don't arum up people together to lollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't arum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to lollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to lollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to lollect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to cong for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # results의 텐서 크기는 (170, 10)\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "\n",
    "    print(predict_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0bb814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
